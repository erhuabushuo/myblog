Title: 用Python写网路爬虫学习笔记一、网络爬虫简介
Date: 2016-11-16 11:55
Category: Python

本章中，我们将会介绍如下主题：

* 网络爬虫领域简介；
* 解释合法性质疑；
* 对目标网站进行背景调研；
* 逐步完善一个高级网络爬虫。


## 网络爬虫何时有用

获取有价值的数据，别人又没有提供API结构化数据。那么你只能爬虫了。


## 网络爬虫是否合法

当抓取的数据是现实生活中的真实数据（比如，营业地址、电话清单）时，是允许转载的。但是，如果是原创数据（比如，意见和评论），通常就会受到版权限制，而不能转载。

## 背景调研

robots.txt和Sitemap，还有**WHOIS**

### 检查rotbots.txt

robots.txt文件定义爬虫限制建议。

    # section 1
    User-agent: BadCrawler
    Disallow: /
    
    # section 2
    User-agent: *
    Crawl-delay: 5 # 5 秒延迟
    Disallow: /trap  # 不允许链接
    
    # section 3
    Sitemap: http://example.webscraping.com/sitemap.xml # 定义Sitemap文件
    
### 检查网站地图

Sitemap文件帮助爬虫定位网站最新内容，而无需爬取每一个网页。

详情定义[http://www.sitemaps.org/protocol.html](http://www.sitemaps.org/protocol.html)

    
    <?xml version="1.0" encoding="UTF-8"?>
    <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
      <url><loc>http://example.webscraping.com/view/Afghanistan-1
        </loc></url>
      <url><loc>http://example.webscraping.com/view/Aland-Islands-2
       </loc></url>
      <url><loc>http://example.webscraping.com/view/Albania-3</loc>
        </url>
      ...
    </urlset>

### 估算网站大小

通过谷歌或百度搜索：site:www.doordu.com
    
    
### 识别网站所用技术

builtwith

    >>> import builtwith
    >>> builtwith.parse("http://www.doordu.com")
    {'javascript-frameworks': ['jQuery', 'MooTools'], 'programming-languages': ['PHP'], 'mobile-frameworks': ['jQuery Mobile'], 'font-scripts': ['Font Awesome'], 'web-servers': ['Nginx'], 'web-frameworks': ['Twitter Bootstrap', 'UIKit'], 'cms': ['Joomla']}
    >>> 

### 寻找网站所有者

    >>> import whois
    >>> print(whois.whois("doordu.com"))
    
    
    {
      "domain_name": [
        "DOORDU.COM",
        "doordu.com"
      ],
      "expiration_date": [
        "2017-03-17 00:00:00",
        "2017-03-17 13:23:48"
      ],
      "zipcode": "518000",
      "status": [
        "ok https://icann.org/epp#ok",
        "ok http://www.icann.org/epp#OK"
      ],
      "city": "shen  zhen",
      "state": "guang dong",
      "whois_server": "grs-whois.hichina.com",
      "address": "Shen Zhen Shi Long Gang Qu Bu Ji Jie Dao Bu Long Lu 18Hao 2Hao Lou 715Shi,,",
      "name": "wangtianxiang",
      "referral_url": "http://www.net.cn",
      "creation_date": [
        "2014-03-17 00:00:00",
        "2014-03-17 13:23:48"
      ],
      "name_servers": [
        "DNS31.HICHINA.COM",
        "DNS32.HICHINA.COM",
        "dns31.hichina.com",
        "dns32.hichina.com"
      ],
      "emails": [
        "wangtx@doordu.com",
        "abuse@list.alibaba-inc.com",
        "support@YinSiBaoHu.AliYun.com"
      ],
      "registrar": "HICHINA ZHICHENG TECHNOLOGY LTD.",
      "dnssec": "unsigned",
      "updated_date": [
        "2014-06-03 00:00:00",
        "2016-08-03 07:09:38"
      ],
      "org": "shenzhen doordu science&technology co.,LTD",
      "country": "CN"
    }


## 编写第一个网络爬虫

爬取(crawing)

* 爬取网站地图；
* 遍历每个网页的数据库ID；
* 跟踪网页链接。

## 下载网页


    import urllib.request
    
    def download(url):
        print("Downloading:", url)
        try:
            html = urllib.request(url).read()
        except urllib.request.URLError as e:
            print("Download error:", e.reason)
            html = None
        return html

### 重试下载


    def download(url, num_retries=2):
        print("Downloading:", url)
        try:
            html = urllib.request.urlopen(url).read()
        except urllib.request.URLError as e:
            print("Download error:", e.reason)
            html = None
            if num_retries > 0:
                if hasattr(e, 'code') and 500 <= e.code < 600:
                    # recursively retry 5xx HTTP errors
                    return download(url, num_retries-1)
        return html


    >>> download("http://httpstat.us/500")
    Downloading: http://httpstat.us/500
    Download error: Internal Server Error
    Downloading: http://httpstat.us/500
    Download error: Internal Server Error
    Downloading: http://httpstat.us/500
    Download error: Internal Server Error

### 设置用户代理

    def download(url, user_agent='wswp', num_retries=2):
        print("Downloading:", url)
        headers = {'User-agent': user_agent}
        try:
            html = urllib.request.urlopen(url).read()
        except urllib.request.URLError as e:
            print("Download error:", e.reason)
            html = None
            if num_retries > 0:
                if hasattr(e, 'code') and 500 <= e.code < 600:
                    # recursively retry 5xx HTTP errors
                    return download(url, user_agent, num_retries-1)
        return html
        
### 网站地图爬虫

<loc>获取URL


    def craw_sitemap(url):
        # download the sitemap file
        sitemap = download(url)
        # extract the sitemap links
        links = re.findall('<loc>(.*)</loc>', sitemap)
        # download each link
        for link in links:
            html = download(link)
            # scrape html here
            

## ID遍历爬虫

* http://example.webscraping.com/view/Afghanistan-1
* http://example.webscraping.com/view/Australia-2
* http://example.webscraping.com/view/Brazil-3

国家名+ID。

忽略国家名只用ID
    
    import itertools
    for page in itertools.count(1):
        url = 'http://example.webscraping.com/view/-%d' % page
        html = download(url)
        if html is None:
            break
        else:
            # success - can scrape the result
            pass


缺陷，缺失的ID会导致退出

    # maximum number of consecutive download errors allowed
    max_errors = 5
    # current number of consecutive download errors
    num_errors = 0
    for page in itertools.count(1):
        url = 'http://example.webscraping.com/view/-%d' % page
        html = download(url)
        if html is None:
            # received an error trying to download this webpage
            num_errors += 1
            if num_errors == max_errors:
                # reached maximum number of
                # consecutive errors so exit
                break
        else:
            # success - can scrape the result
            # ...
            num_errors = 0
            
### 链接爬虫

跟踪所有链接的方式

    
    def link_crawler(seed_url, link_regex):
        """Crawl from the given seed URL following
        links matched by link_regex
        """
        crawl_queue = [seed_url]
        while crawl_queue:
            url = crawl_queue.pop()
            html = download(url)
            # filter for links matching our regular expression
            for link in get_links(html):
                if re.match(link_regex, link):
                    crawl_queue.append(link)
    
    
    def get_links(html):
        """Return a list of links from html
        """
        # a regular expression to extract all links from the webpage
        webpage_regex = re.compile('<a[^>]+href=["\'](.*?)["\']',
                                   re.IGNORECASE)
        # list of all links from the webpage
        return webpage_regex.findall(html)
        
对于示例网站，我们想要爬取的是国家列表索引页和国家页面。其中，索引页链接格式如下。

* http://example.webscraping.com/index/1
* http://example.webscraping.com/index/2

国家页链接格式如下。

* http://example.webscraping.com/view/Afghanistan-1
* http://example.webscraping.com/view/Aland-Islands-2


    >>> link_crawler('http://example.webscraping.com',
        '/(index|view)')
    Downloading: http://example.webscraping.com
    Downloading: /index/1
    Traceback (most recent call last):
        ...
    ValueError: unknown url type: /index/1
    
解决上面相对链接问题

    def link_crawler(seed_url, link_regex):
        """Crawl from the given seed URL following
        links matched by link_regex
        """
        crawl_queue = [seed_url]
        while crawl_queue:
            url = crawl_queue.pop()
            html = download(url)
            # filter for links matching our regular expression
            for link in get_links(html):
                if re.match(link_regex, link):
                    link = urllib.parse.urljoin(seed_url, link)
                    crawl_queue.append(link)
                    
                    
解决重复链接循环问题

    def link_crawler(seed_url, link_regex):
        """Crawl from the given seed URL following
        links matched by link_regex
        """
        crawl_queue = [seed_url]
        # keep track which URL's have seen before
        seen = set(crawl_queue)
        while crawl_queue:
            url = crawl_queue.pop()
            html = download(url)
            # filter for links matching our regular expression
            for link in get_links(html):
                if re.match(link_regex, link):
                    link = urllib.parse.urljoin(seed_url, link)
                    # check if have already seen this link
                    if link not in seen:
                        seen.add(link)
                        crawl_queue.append(link)

**高级功能**

解析robots.txt

    >>> from urllib import robotparser
    >>> rp = robotparser.RobotFileParser()
    >>> rp.set_url('http://example.webscraping.com/robots.txt')
    >>> rp.read()
    >>> url = 'http://example.webscraping.com'
    >>> user_agent = 'BadCrawler'
    >>> rp.can_fetch(user_agent, url)
    False
    >>> user_agent = 'GoodCrawler'
    >>> rp.can_fetch(user_agent, url)
    True
    
集成到爬虫中，需要在crawl循环中添加该检查。

    ...
    while crawl_queue:
        url = crawl_queue.pop()
        # check url passes robots.txt restrictions
        if rp.can_fetch(user_agent, url):
            ...
        else:
            print('Blocked by robots.txt:', url)
            

支持代理

    proxy = ...
    opener = urllib.request.build_opener()
    proxy_params = {urllib.parse.urlparse.urlparse(url).scheme: proxy}
    opener.add_handler(urllib.request.ProxyHandler(proxy_params))
    response = opener.open(request)
    
    
    def download(url, user_agent='superman', proxy=None, num_retries=2):
        print("Downloading:", url)
        headers = {'User-agent': user_agent}
        request = urllib.request.Request(url, headers=headers)
        opener = urllib.request.build_opener()
        if proxy:
            proxy_params = {urllib.parse.urlparse(url).scheme: proxy}
            opener.add_handler(urllib.request.ProxyHandler(proxy_params))
    
        try:
            html = opener.open(request).read()
        except urllib.request.URLError as e:
            print("Download error:", e.reason)
            html = None
            if num_retries > 0:
                if hasattr(e, 'code') and 500 <= e.code < 600:
                    # recursively retry 5xx HTTP errors
                    return download(url, user_agent, num_retries-1)
        return html
        
下载限速

    class Throttle:
        """Add a delay between downloads to the same domain
        """
        def __init__(self, delay):
            # amount of delay between downloads for each domain
            self.delay = delay
            # timestamp of when a domain was last accessed
            self.domains = {}
    
        def wait(self, url):
            domain = urllib.parse.urlparse(url).netloc
            last_accessed = self.domains.get(domain)
    
            if self.delay > 0 and last_accessed is not None:
                sleep_secs = self.delay - (datetime.datetime.now() -
                                           last_accessed).seconds
                if sleep_secs > 0:
                    # domain has been accessed recently
                    # so need to sleep
                    time.sleep(sleep_secs)
            # update the last accesed time
            self.domains[domain] = datetime.datetime.now()
            
避免爬虫陷阱

    def link_crawler(..., max_depth=2):
        max_depth = 2
        seen = {}
        ...
        depth = seen[url]
        if depth != max_depth:
            for link in links:
                if link not in seen:
                    seen[link] = depth + 1
                    crawl_queue.append(link)