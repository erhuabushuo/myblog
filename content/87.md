Title: [Web scraping with python]：介绍 Web Scraping
Date: 2017-04-20 11:55
Category: Python

# 介绍 Web Scraping

网页数据采集广泛用在众多领域，比如数据分析、信息安全等。

## 什么时候要 Web Scraping

* 从多个站点收集数据
* 网站没有提供API方式获取数据
* 一些API并没有与网站同步更新

## 采集数据合法码？

有人个人用当然没有问题，但是如果是转载就要谨慎了。

从世界各地法院案件来说，当抓取的数据是现实生活中 的真实数据（比如，营业地址、电话清单)时，是允许转载的。但是，如果是原创数据（比如，意见和评论），通常就会受到版权限制，而不能转载。

## 背景调查

我们首先需要对目标站点的规模和结构进行一定程度的了解。网站的robots.txt和sitemap文件可以给予我们一定帮助，还有一些外部工具例如Google和WOIS也能提供一些信息。

### 检查robots.txt

[http://www.robotstxt.org](http://www.robotstxt.org)

    # section 1
    User-agent: BadCrawler
    Disallow: /
    
    # section 2
    User-agent: *
    Crawl-delay: 5
    Disallow: /trap
    
    # section 3
    Sitemap: http://example.webscraping.com/sitemap.xml
    

### 检查Sitemap

[http://www.sitemaps.org/protocol.html](http://www.sitemaps.org/protocol.html)

    <?xml version="1.0" encoding="UTF-8"?>
    <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
      <url><loc>http://example.webscraping.com/view/Afghanistan-1
        </loc></url>
      <url><loc>http://example.webscraping.com/view/Aland-Islands-2
       </loc></url>
      <url><loc>http://example.webscraping.com/view/Albania-3</loc>
        </url>
      ...
    </urlset>
    
### 估算网站大小

![](http://i2.muimg.com/1949/346640334d56d394.png)

### 识别网站使用技术

使用builtwith

    builtwith("http://www.doordu.com")
    {'cms': ['Joomla'],
     'font-scripts': ['Font Awesome'],
     'javascript-frameworks': ['MooTools', 'jQuery'],
     'mobile-frameworks': ['jQuery Mobile'],
     'programming-languages': ['PHP'],
     'web-frameworks': ['UIKit', 'Twitter Bootstrap'],
     'web-servers': ['Apache']}

### 找出网站所有者

    >>> import whois
    >>> whois.whois('doordu.com'))
    {'address': 'Shen Zhen Shi Nan Shan Qu Tao Yuan Jie Dao Liu Xian Da Dao Nan '
                'Shan Yun Gu Er Qi Ba Dong 4Lou 4BShi',
     'city': 'shen  zhen',
     'country': 'CN',
     'creation_date': [datetime.datetime(2014, 3, 17, 0, 0),
                       datetime.datetime(2014, 3, 17, 13, 23, 48)],
     'dnssec': 'unsigned',
     'domain_name': ['DOORDU.COM', 'doordu.com'],
     'emails': ['wangtx@doordu.com',
                'DomainAbuse@service.aliyun.com',
                'support@YinSiBaoHu.AliYun.com'],
     'expiration_date': [datetime.datetime(2020, 3, 17, 0, 0),
                         datetime.datetime(2020, 3, 17, 13, 23, 48)],
     'name': 'wangtianxiang',
     'name_servers': ['VIP1.ALIDNS.COM',
                      'VIP2.ALIDNS.COM',
                      'vip1.alidns.com',
                      'vip2.alidns.com'],
     'org': 'Shen Zhen Shi Duo Du  Ke Ji You Xian Gong Si',
     'referral_url': 'http://www.net.cn',
     'registrar': 'HICHINA ZHICHENG TECHNOLOGY LTD.',
     'state': 'guang dong',
     'status': ['ok https://icann.org/epp#ok', 'ok http://www.icann.org/epp#OK'],
     'updated_date': [datetime.datetime(2017, 2, 21, 0, 0),
                      datetime.datetime(2017, 2, 21, 5, 38, 32)],
     'whois_server': 'grs-whois.hichina.com',
     'zipcode': '518000'}

## 编写第一个网络爬虫

下载感兴趣的网页称为crawling

* 爬取网站地图
* 遍历每个网页的数据库ID
* 跟踪网页


### Scraping和Crawling

* Web scraper: 对特定网站特定数据进行抓取，当网站变更时也要进行修改
* Web crawler: 通用方法，用来通过顶级域名并且跟踪链接方式来抓取一段数据

### 下载网页

    import urllib.request
    from urllib.error import URLError, HTTPError, ContentTooShortError
    
    
    def download(url):
        print("下载：", url)
        try:
            html = urllib.request.urlopen(url).read()
        except (URLError, HTTPError, ContentTooShortError) as e:
            print("下载错误：", e.reason)
            html = None
            
        return(html)    

### 下载重试


    def download(url, num_retries=2):
        print("下载：", url)
        try:
            html = urllib.request.urlopen(url).read()
        except (URLError, HTTPError, ContentTooShortError) as e:
            print("下载错误：", e.reason)
            html = None
            if num_retries > 2:
                if hasattr(e, 'code') and 500 <= e.code < 600:
                    # 如果5xx HTTP错误时递归重试
                    return download(url, num_retries - 1)
            
        return(html)    


    >>> from ex02 import download
    >>> download('http://httpstat.us/500')
    下载： http://httpstat.us/500
    下载错误： Internal Server Error
    下载： http://httpstat.us/500
    下载错误： Internal Server Error
    下载： http://httpstat.us/500
    下载错误： Internal Server Error
    >>> 
    
### 设置user agent


    def download(url, num_retries=2, user_agent='snaker'):
        print("下载：", url)
        request = urllib.request.Request(url)
        request.add_header('User-agent', user_agent)
        try:
            html = urllib.request.urlopen(request).read()
        except (URLError, HTTPError, ContentTooShortError) as e:
            print("下载错误：", e.reason)
            html = None
            if num_retries > 0:
                if hasattr(e, 'code') and 500 <= e.code < 600:
                    # 如果5xx HTTP错误时递归重试
                    return download(url, num_retries - 1)
            
        return(html) 
        
### Sitemap抓取

    import urllib.request
    from urllib.error import URLError, HTTPError, ContentTooShortError
    import re
    
    
    def download(url, num_retries=2, user_agent='snaker', charset='utf-8'):
        print("下载：", url)
        request = urllib.request.Request(url)
        request.add_header('User-agent', user_agent)
        try:
            resp = urllib.request.urlopen(request)
            cs = resp.headers.get_content_charset()
            if not cs:
                cs = charset
            html = resp.read().decode(cs)
        except (URLError, HTTPError, ContentTooShortError) as e:
            print("下载错误：", e.reason)
            html = None
            if num_retries > 0:
                if hasattr(e, 'code') and 500 <= e.code < 600:
                    # 如果5xx HTTP错误时递归重试
                    return download(url, num_retries - 1)
        return html
    
    
    def crawl_sitemap(url):
        # 下载sitemap文件
        sitemap = download(url)
        # 解开所有链接地址
        links = re.findall('<loc>(.*?)</loc>', sitemap)    
        # 下载每个链接
        for link in links:
            html = download(link)
            # 抓取html
            # ...

### ID遍历抓取


	def crawl_site(url):
	    for page in itertools.count(2):
		pg_url = '{}{}'.format(url, page)
		html = download(pg_url)
		if html is None:
		    break
		# 成功，抓取结果

	>>> crawl_site('https://www.doordu.com/index.php/zh/')
	下载： https://www.doordu.com/index.php/zh/2
	下载： https://www.doordu.com/index.php/zh/3
	下载： https://www.doordu.com/index.php/zh/4
	下载错误： Not Found
	
	def crawl_site(url, max_errors=5):
	    for page in itertools.count(1):
		pg_url = '{}{}'.format(url, page)
		html = download(pg_url)
		if html is None:
		    number_errors += 1
		    if number_errors == max_errors:
		        # 达到最大错误次数，退出
		        break
		else:
		    num_errors = 0
		    
### 链接抓取

	def link_crawler(start_url, link_regex):
	    """
	    在给定URL里使用link_regex匹配链接追踪抓取
	    """
	    crawl_queue = [start_url]
	    seen = set(crawl_queue)
	    while crawl_queue:
		url = crawl_queue.pop()
		html = download(url)
		if not html:
		    continue
		for link in get_links(html):
		    if re.match(link_regex, link):
		        # 拼接基础URL
		        abs_link = urljoin(start_url, link)
		        if abs_link not in seen:
		            # 如果地址没有出现过
		            seen.add(abs_link)
		            crawl_queue.append(abs_link)


	def get_links(html):
	    webpage_regex = re.compile("""<a[^>]+href=["'](.*?)["']""", re.IGNORECASE)
	    return webpage_regex.findall(html)

#### 高级特性

##### 解析 robots.txt

	>>> from urllib import robotparser
	>>> rp = robotparser.RobotFileParser()
	>>> rp.set_url('https://www.baidu.com/robots.txt')
	>>> rp.read()
	>>> url = 'https://www.baidu.com/s?wd=doordu'
	>>> user_agent = 'BadCrawler'
	>>> rp.can_fetch(user_agent, url)
	False



		def link_crawler(start_url, link_regex, robots_url=None, user_agent='doordu'):
		    """
		    在给定URL里使用link_regex匹配链接追踪抓取
		    """
		    if not robots_url:
			robots_url = '{}/robots.txt'.format(start_url)
		    rp = get_robots_parser(robots_url)
		    crawl_queue = [start_url]
		    seen = set(crawl_queue)
		    while crawl_queue:
			url = crawl_queue.pop()
			if rp.can_fetch(user_agent, url):
			    html = download(url)
			    if not html:
				continue
			    for link in get_links(html):
				if re.match(link_regex, link):
				    # 拼接基础URL
				    abs_link = urljoin(start_url, link)
				    if abs_link not in seen:
				        # 如果地址没有出现过
				        seen.add(abs_link)
				        crawl_queue.append(abs_link)
			else:
			    print('Blocked by robots.txt:', url)


		def get_links(html):
		    webpage_regex = re.compile("""<a[^>]+href=["'](.*?)["']""", re.IGNORECASE)
		    return webpage_regex.findall(html)


		def get_robots_parser(robots_url):
		     rp = robotparser.RobotFileParser()
		     rp.set_url(robots_url)
		     rp.read()
		     return rp

##### 使用代理

通过代理来请求

	proxy = 'http://myproxy.net:1234' # example string 
	proxy_support = urllib.request.ProxyHandler({'http': proxy})
	opener = urllib.request.build_opener(proxy_support)
	urllib.request.install_opener(opener) 
	# now requests via urllib.request will be handled via proxy



	def download(url, num_retries=2, user_agent='snaker', charset='utf-8', proxy=None):
	    print("下载：", url)
	    request = urllib.request.Request(url)
	    request.add_header('User-agent', user_agent)
	    try:
		if proxy:
		    proxy_support = urllib.request.ProxyHandler({'http': proxy})
		    opener = urllib.request.build_opener(proxy_support)
		    urllib.request.install_opener(opener)
		resp = urllib.request.urlopen(request)
		cs = resp.headers.get_content_charset()
		if not cs:
		    cs = charset
		html = resp.read().decode(cs)
	    except (URLError, HTTPError, ContentTooShortError) as e:
		print("下载错误：", e.reason)
		html = None
		if num_retries > 0:
		    if hasattr(e, 'code') and 500 <= e.code < 600:
		        # 如果5xx HTTP错误时递归重试
		        return download(url, num_retries - 1)
	    return html

##### Throttling下载

如果我们请求下载太频繁，我们有可能会被blocked掉，通过加入Throttling来降低访问频率


	from urllib.parse import urlparse
	import time

	class Throttle:
	    def __init__(self, delay):
		self.delay = delay
		self.domains = {}

	    def wait(self, url):
		domain = urlparse(url).netloc
		last_accessed = self.domains.get(domain)

		if self.delay > 0 and last_accessed is not None:
		    sleep_secs = self.delay - (time.time() - last_accessed)
		    if sleep_secs > 0:
		        time.sleep(sleep_secs)
		self.domains[domain] = time.time()
		
##### 避免爬虫陷阱

当前我们的抓取程序会随着链接继续抓取，但是有的页面会根据动态参数无限制的生成内容，对着我们通过对添加深度(depth)参数避免。

#### 最终版本

最终版本我们使用requests库

	import re
	from urllib import robotparser
	from urllib.parse import urljoin

	import requests
	from chp1.throttle import Throttle


	def download(url, user_agent='wswp', num_retries=2, proxies=None):
	    """ Download a given URL and return the page content
		args:
		    url (str): URL
		kwargs:
		    user_agent (str): user agent (default: wswp)
		    proxies (dict): proxy dict w/ keys 'http' and 'https', values
		                    are strs (i.e. 'http(s)://IP') (default: None)
		    num_retries (int): # of retries if a 5xx error is seen (default: 2)
	    """
	    print('Downloading:', url)
	    headers = {'User-Agent': user_agent}
	    try:
		resp = requests.get(url, headers=headers, proxies=proxies)
		html = resp.text
		if resp.status_code >= 400:
		    print('Download error:', resp.text)
		    html = None
		    if num_retries and 500 <= resp.status_code < 600:
		        # recursively retry 5xx HTTP errors
		        return download(url, num_retries - 1)
	    except requests.exceptions.RequestException as e:
		print('Download error:', e)
		html = None
	    return html


	def get_robots_parser(robots_url):
	    " Return the robots parser object using the robots_url "
	    rp = robotparser.RobotFileParser()
	    rp.set_url(robots_url)
	    rp.read()
	    return rp


	def get_links(html):
	    """ Return a list of links (using simple regex matching)
		from the html content """
	    # a regular expression to extract all links from the webpage
	    webpage_regex = re.compile("""<a[^>]+href=["'](.*?)["']""", re.IGNORECASE)
	    # list of all links from the webpage
	    return webpage_regex.findall(html)


	def link_crawler(start_url, link_regex, robots_url=None, user_agent='wswp',
		         proxies=None, delay=3, max_depth=4):
	    """ Crawl from the given start URL following links matched by link_regex.
	    In the current implementation, we do not actually scrape any information.

		args:
		    start_url (str): web site to start crawl
		    link_regex (str): regex to match for links
		kwargs:
		    robots_url (str): url of the site's robots.txt
		                      (default: start_url + /robots.txt)
		    user_agent (str): user agent (default: wswp)
		    proxies (dict): proxy dict w/ keys 'http' and 'https', values
		                    are strs (i.e. 'http(s)://IP') (default: None)
		    delay (int): seconds to throttle between requests
		                 to one domain (default: 3)
		    max_depth (int): maximum crawl depth (to avoid traps) (default: 4)
	    """
	    crawl_queue = [start_url]
	    # keep track which URL's have seen before
	    seen = {}
	    if not robots_url:
		robots_url = '{}/robots.txt'.format(start_url)
	    rp = get_robots_parser(robots_url)
	    throttle = Throttle(delay)
	    while crawl_queue:
		url = crawl_queue.pop()
		# check url passes robots.txt restrictions
		if rp.can_fetch(user_agent, url):
		    depth = seen.get(url, 0)
		    if depth == max_depth:
		        print('Skipping %s due to depth' % url)
		        continue
		    throttle.wait(url)
		    html = download(url, user_agent=user_agent, proxies=proxies)
		    if not html:
		        continue
		    # TODO: add actual data scraping here
		    # filter for links matching our regular expression
		    for link in get_links(html):
		        if re.match(link_regex, link):
		            abs_link = urljoin(start_url, link)
		            if abs_link not in seen:
		                seen[abs_link] = depth + 1
		                crawl_queue.append(abs_link)
		else:
		    print('Blocked by robots.txt:', url)


